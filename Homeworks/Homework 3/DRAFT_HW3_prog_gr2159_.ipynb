{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3 - Logistic Regression (50 points)\n",
    "\n",
    "For this assignment, you will be using the Breast Cancer Wisconsin  dataset to create a classifier that can help diagnose patients. \n",
    "\n",
    "You task is to determine if you can use logistic regression with the features to predict if a tumor is benign or malignant.  This is an important task, as if save lives.\n",
    "\n",
    "``Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.''\n",
    "\n",
    "The ten real-valued features compute different measurements on the cell nucleus.  The official documentation describes the features as: \n",
    "\n",
    "*  radius (mean of distances from center to points on the perimeter)\n",
    "* texture (standard deviation of gray-scale values)\n",
    "* perimeter\n",
    "* area\n",
    "* smoothness (local variation in radius lengths)\n",
    "* compactness (perimeter^2 / area - 1.0)\n",
    "* concavity (severity of concave portions of the contour)\n",
    "* concave points (number of concave portions of the contour)\n",
    "* symmetry\n",
    "* fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "For this semester, the teaching staff of this course will be using Autograder to grade programming assignment. Here are three things we would like you to know before starting. `PLEASE READ CAREFULLY.` Otherwise, you might lose points on some questions.\n",
    "\n",
    "* If you see any blocks containing statements like `grader.check(\"Qxx\")`, please `do not modify` them. You can add new cells to the notebook, but just make sure there is `no other cells` between the answer cells containing tag `# TODO Qxx` and grading cells like 'grader.check(\"Qxx\")`. \n",
    "\n",
    "* If the instructions say that you are required to use certain names for output variables, please `follow the instructions`, and you are not supposed to change the names of any given variables. You can still create new variables, but don't forget to `assign the output variables to correct values`. If the `type` of a output variable is specified, make sure the type of the variable is correct.\n",
    "\n",
    "* You can use print statements to print out results through out the notebook. However, if you have any `print statements within functions`, please make sure putting them `in comments` before you submit.\n",
    "\n",
    "* Please note for questions that require you to plot, please **_DO NOT MODIFY_** statements like `plt.show(block=False)`. Changing the statement would block the execution of autograder and you might lose points on that question.\n",
    "\n",
    "* Please `APPEND YOUR NYU NETID` to the name your submission (for example, name your submission as \"HW1_prog_abc12345.ipynb\" when you submit on Gradescope, and replace <abc1234> with your NYU NetID). \n",
    "\n",
    "Good luck with programming assignment 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Important Libraries\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer # taking included data set from Sklearn http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "from sklearn.linear_model import LogisticRegression # importing Sklearn's logistic regression's module\n",
    "from sklearn import preprocessing # preprossing is what we do with the data before we run the learning algorithm\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "# import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data set.\n",
    "\n",
    "In the below code cell, you will load the data from sklearn using the method given. Check import statements and use the given function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q01\n",
    "cancer = load_breast_cancer()   # type in load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-postage",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q01 - cancer.target.shape:  (569,)\n",
      "Q01 - cancer.data.shape:  (569, 30)\n"
     ]
    }
   ],
   "source": [
    "# VERIFY - Print the shape of data and target\n",
    "print('Q01 - cancer.target.shape: ', cancer.target.shape)\n",
    "print('Q01 - cancer.data.shape: ', cancer.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "# Read through the description of the dataset by uncommenting the line of code below\n",
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "Scale after splitting the data into train and test since we will be using gradient ascent. \n",
    "* Use `train_test_split` to split the data (`75% train` and `25% test`) to `X_train`, `X_test`, `y_train`, `y_test` with `random_state` of 42\n",
    "* Reshape `y_train` into 2D array `y_2d_train` and `y_test` into 2D array `y_2d_test`\n",
    "* Use `preprocessing` to scale the data.  Remember to scale the training data first and then using the same method scale the test dataset.\n",
    "* Augment the dataset with a column of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q02\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.25, random_state=42)\n",
    "\n",
    "y_2d_train = y_train.reshape(-1,1)\n",
    "y_2d_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-hepatitis",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q02 - X_train.shape:  (426, 30)\n",
      "Q02 - y_2d_train.shape:  (426, 1)\n"
     ]
    }
   ],
   "source": [
    "# VERIFY - Print the shape of X_train and y_2d_train\n",
    "print('Q02 - X_train.shape: ', X_train.shape)\n",
    "print('Q02 - y_2d_train.shape: ', y_2d_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q02 - cancer.feature_names:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# VERIFY - Printing the names of all the features\n",
    "print('Q02 - cancer.feature_names: ', cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q03\n",
    "# Append a column of ones to X_train\n",
    "# ones is a  vector of shape n,1\n",
    "ones = np.ones((X_train.shape[0],1))\n",
    "# Append a column of ones in the beginning of X_train an save in variable X_train_1(<np.ndarray>).\n",
    "X_train_1 = np.hstack((ones, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-species",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q03 - X_train_1.shape:  (426, 31)\n",
      "Q03 - X_train_1:  [[ 1.      12.89    13.12    ...  0.05366  0.2309   0.06915]\n",
      " [ 1.      13.4     20.52    ...  0.2051   0.3585   0.1109 ]\n",
      " [ 1.      12.96    18.29    ...  0.06608  0.3207   0.07247]\n",
      " ...\n",
      " [ 1.      14.29    16.82    ...  0.03333  0.2458   0.0612 ]\n",
      " [ 1.      13.98    19.62    ...  0.1827   0.3179   0.1055 ]\n",
      " [ 1.      12.18    20.52    ...  0.07431  0.2694   0.06878]]\n"
     ]
    }
   ],
   "source": [
    "# VERIFY\n",
    "print('Q03 - X_train_1.shape: ', X_train_1.shape)\n",
    "print('Q03 - X_train_1: ', X_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression Using Gradient Ascent\n",
    "\n",
    "You will perform the following steps:\n",
    "* write the sigmoid function $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "* initialize ${\\bf w}$\n",
    "* prediction: write the function to compute the probability of every example in $X$ belonging to class one\n",
    "* write the log likelihood function (see lecture notes for the formula)\n",
    "* write the gradient ascent algorithm\n",
    "* plot the likelihood v/s the number of iterations\n",
    "* predict the class label (i.e. $0,1$) for every example in $X$ for a given ${\\bf w}$ and $t$\n",
    "* Evaluate your hypothesis by using your hypothesis to predict the class of the examples in the test set.  Using these predicted value you will then determine the precision, recall and F1 score of the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q04\n",
    "# Write the sigmoid function\n",
    "def sigmoid(z):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tobacco",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY - Sigmoid of 0 should be equal to half\n",
    "print('Q04 - sigmoid(0): ', sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ${\\bf w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q05\n",
    "# Initialize w_init to a zero matrix with shape (X_train_1.shape[1],1)\n",
    "w_init = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-chile",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY\n",
    "print('Q05 - w_init.shape: ', w_init.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Finish writing the function, `hypothesis`, that computes the probability of each example in $X$ belonging to class one.  (i.e. $\\hat{\\bf y}=\\sigma(X{\\bf w})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q06\n",
    "# Write the hypothesis function which assumes the design matrix X is augmented with a column of ones\n",
    "def hypothesis(X, w):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-corps",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q07 \n",
    "# Compute y_hat(<np.ndarray>) using your hypotheis function with arguments X_train_1 and w_init\n",
    "y_hat_init = hypothesis(X_train_1,w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-outdoors",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY\n",
    "# print('Q07 - y_hat_init: ', y_hat_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood Function\n",
    "Write the code to calculate the log likelihood as discussed in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q08\n",
    "# Write the log likelihood function\n",
    "def log_likelihood(X, y, w):\n",
    " \n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-punch",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY - The value should be equal to -295.2806989185367 using X_train_1, y_2d_train, w, X_train_1.shape[0].\n",
    "print('Q08 - likelihood: ', log_likelihood(X_train_1, y_2d_train, w_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q09\n",
    "# Write the gradient ascent function\n",
    "def Gradient_Ascent(X, y, learning_rate, num_iters):\n",
    "    # We assume X has been augmented with a column of ones\n",
    "    \n",
    "    # Number of training examples.\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Initialize w(<np.ndarray>). Zeros vector of shape X.shape[1],1\n",
    "    w = np.zeros((X.shape[1],1))\n",
    "    \n",
    "    # Initiating list to store values of likelihood(<list>) after few iterations.\n",
    "    log_likelihood_values = []\n",
    "    \n",
    "    # Gradient Ascent - local optimization technique\n",
    "    ...\n",
    "        \n",
    "        # Computing log likelihood of seeing examples for current value of w\n",
    "        if (i % 10) == 0:\n",
    "            log_likelihood_values.append(log_likelihood(X, y, w))\n",
    "            print(log_likelihood(X, y, w))\n",
    "        \n",
    "    return w, log_likelihood_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-intensity",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please try many different values for the learning rate (including very small values).\n",
    "learning_rate = ...\n",
    "num_iters = ...\n",
    "# Calculate w and likelihood values using Gradient_Ascent with X_train_1, y_2d_train\n",
    "w, log_likelihood_values = Gradient_Ascent(X_train_1, y_2d_train, learning_rate, num_iters)\n",
    "print(w, log_likelihood_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-injection",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Likelihood v/s Number of Iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to plot Likelihood v/s Number of Iterations.\n",
    "iters = np.array(range(0,num_iters,10))\n",
    "plt.plot(iters,log_likelihood_values,'.-',color='green')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title(\"Log-Likelihood vs Number of Iterations.\")\n",
    "plt.grid()\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the likelihood increasing as number of Iterations increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the class label for every example in $X$ for a given ${\\bf w}$ and $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Given a set of examples write the function to compute predicted which class for each example: 0 if the probability of belonging to class  is < t and returns 1 otherwise) - 10 points\n",
    "def predict_class(X, w, t):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-breakdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall and F1: Evaluating your hypothesis using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q12\n",
    "# Preidct the class y_hat using X_test and w you just calculated if the threshold is t = 0.5\n",
    "\n",
    "# First augment the test dataset with a column of ones.\n",
    "ones = ...\n",
    "X_test_1 = ...\n",
    "# Now predict the label of each example in your test set\n",
    "y_hat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-specific",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q13\n",
    "# Write the precision_recall function by first calculating: false_pos, false_neg and true_pos.  Using these numbers compute the precision and recall\n",
    "def precision_recall(y_hat, y, threshold):  \n",
    "\n",
    "    # Calculate false positive and false negative\n",
    "    # HINT: if done correctly, false_pos should be 1 and false_neg should be 1\n",
    "    false_pos = ...\n",
    "    false_neg = ...\n",
    "\n",
    "    # Calculate true positive and true negatives\n",
    "    # HINT: if done correctly, true_pos should be 88\n",
    "    true_pos = ...\n",
    "\n",
    "    precision = ...\n",
    "    recall = ...\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-earth",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q14\n",
    "# Calculate precision and recall using on the test data where the threshold is 0.5\n",
    "\n",
    "precision, recall = ...\n",
    "\n",
    "print('Q14 - precision: ', precision)\n",
    "print('Q14 - recall: ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-copper",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q15\n",
    "# Write the F1_score function\n",
    "def f1_score(precision, recall):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the F1 score on the test data set using the precision and recall you computed above.\n",
    "f1_score(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-blast",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn's implementation of Logistic regression\n",
    "\n",
    "Next, use Sklearn's implementation of Logistic regression.  Once you have your hypothesis you will use your model on the test data and then evaluate how well it did using Sklearn's built in functions to compute the accuracy, precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Model using Sklearn Library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create object of logistic regression model. So we don't use any regularization, we can set the penalty to `none` or set C to a very large value (for example, C = 100000000), \n",
    "# to make lambda (C = 1/lambda) nearly 0.\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(penalty = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-plastic",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q17\n",
    "# Fit the model\n",
    "# Don't use matrix X_train_1. Instead, use X_train.\n",
    "logreg.fit(..., ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-israeli",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: Q19\n",
    "manual: true\n",
    "points:\n",
    "  each: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q18\n",
    "# Print out all the coefficients\n",
    "w_logreg = ...\n",
    "intercept_logreg = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-trauma",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-noise",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY - Compare the parameters computed by logreg model and gradient ascent. They should be nearly same.\n",
    "print('Q18 - w_logreg: ', w_logreg)\n",
    "print('Q18 - intercept_logreg: ', intercept_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measure: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q19\n",
    "# Find the predicted values on test set (X_test not X_test_1) using logreg.predict\n",
    "y_hat_logreg = ...\n",
    "\n",
    "# Find the accuracy achieved on test set using logreg.score and y_test \n",
    "acc_logreg = ...\n",
    "\n",
    "print(\"Q19 - Accuracy on training data = %f\" % acc_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-delight",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics: precision, recall, F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# TODO Q20\n",
    "# Find Precision, recall and fscore using precision_recall_fscore_support nethod of sklearn\n",
    "# Using y_test and y_hat_logreg\n",
    "prec, recal, fscore, sup = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-dealer",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY\n",
    "print('Q20 - prec: ', prec)\n",
    "print('Q20 - recal: ', recal)\n",
    "print('Q20 - fscore: ', fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!  Run your gradient ascent algorithm without scaling the training dataset.  \n",
    "What did you notice.  Describe the best hyperparamters  you found (i.e. `learning_rate`, and `num_iters`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
